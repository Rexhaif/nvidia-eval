{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils import data as D\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from data_utils.preprocess import process_tweet, batch_tokens\n",
    "from data_utils.tokenization import SentencePieceTokenizer\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = torch.load(\"lm.pth\", map_location=device)\n",
    "teacher.aux_lm_loss = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SentencePieceTokenizer(model_path=\"../models/ama_32k_tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationDataset(D.Dataset):\n",
    "    def __init__(self, text_csv_file: str, text_column: str, tokenizer: SentencePieceTokenizer, preprocess_fn):\n",
    "        super(DistillationDataset, self).__init__()\n",
    "        self.table: pd.Dataset = pd.read_csv(text_csv_file, memory_map=True)\n",
    "        \n",
    "        self.column = text_column\n",
    "        self.tokenizer = tokenizer\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "     \n",
    "    def tokenize(self, tokenizer, text):\n",
    "        \"\"\"\n",
    "        Tokenizes a text using SentencePiece tokenizer\n",
    "        \"\"\"\n",
    "        input_text = self.tokenizer.EncodeAsIds(text, self.preprocess_fn).tokenization\n",
    "    \n",
    "        return input_text\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_collate_fn():\n",
    "        def batch_tokens(token_lists, tensor_type=torch.LongTensor, fill_value=0):\n",
    "            lens = torch.from_numpy(np.array(list(map(len, token_lists)), dtype=np.int64))\n",
    "            batch_tensor = fill_value * torch.ones(len(lens), max(lens)).type(tensor_type)\n",
    "            for i, string in enumerate(token_lists):\n",
    "                _tokenize_str(string, tensor_type, batch_tensor[i])\n",
    "            return batch_tensor.permute(1, 0), lens - 1\n",
    "\n",
    "        def _tokenize_str(data, tensor_type, char_tensor=None):\n",
    "            \"\"\"\n",
    "            Parses a utf-8 encoded string and assigns to ByteTensor char_tensor.\n",
    "            If no char_tensor is provide one is created.\n",
    "            Typically used internally by `tokenize_str_batch`.\n",
    "            \"\"\"\n",
    "            if char_tensor is None:\n",
    "                if isinstance(data, str):\n",
    "                    # data could either be a string or a list of ids.\n",
    "                    data = data.encode()\n",
    "                char_tensor = tensor_type(len(data))\n",
    "            for i, char in enumerate(data):\n",
    "                char_tensor[i] = char\n",
    "                \n",
    "        return batch_tokens\n",
    "\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.table)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> List[str]:\n",
    "        sample = self.table[self.column].iloc[idx]\n",
    "        sample = self.tokenize(self.tokenizer, sample)\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 768, padding_idx=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher.encoder.encoder.embed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset = DistillationDataset(\"../data/test.csv\", 'Tweet', tokenizer=tokenizer, preprocess_fn=process_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = D.Subset(base_dataset, np.arange(0, 9500000)), \\\n",
    "                     D.Subset(base_dataset, np.arange(9500000, 9750000)), \\\n",
    "                     D.Subset(base_dataset, np.arange(9750000, len(base_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = D.DataLoader(base_dataset, batch_size=10, collate_fn=base_dataset.get_collate_fn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt, ln = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([33, 10]), torch.Size([10]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt.shape, ln.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([29, 26,  8, 19, 31, 32, 19, 18, 27, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    feats, lm = teacher(txt, ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = torch.softmax(lm, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_lm = lm[:, :1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 1, 32001])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_lm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(tensor, true_value, example_id=0, tokenizer = tokenizer):\n",
    "    sequence = tensor[:, example_id]\n",
    "    true_value = tokenizer.EncodeAsIds(true_value).tokenization\n",
    "    print(sequence.shape)\n",
    "    for i, dist in enumerate(sequence):\n",
    "        idx   = torch.argmax(dist)\n",
    "        value = torch.max(dist)\n",
    "        if idx != 0:\n",
    "            token = tokenizer.DecodeIds(idx.item())\n",
    "        else:\n",
    "            token = \"<pad>\"\n",
    "        if i < len(true_value):\n",
    "            print(f\"{token:^20} [{value.item():.4f}] | {tokenizer.DecodeIds(true_value[i]):^20} [{dist[true_value[i]]:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 32001])\n",
      "         ⁇           [0.9977] |                      [0.0000]\n",
      "         '           [0.1075] |          ⁇           [0.0000]\n",
      "         .           [0.0675] |          P           [0.0006]\n",
      "        ISH          [0.2197] |          OL          [0.0085]\n",
      "        ICAL         [0.5260] |          IT          [0.0003]\n",
      "         IT          [0.2743] |          IC          [0.0032]\n",
      "         ⁇           [0.0936] |          O           [0.0007]\n",
      "         ⁇           [0.0693] |          E           [0.0015]\n",
      "         an          [0.9228] |        urope         [0.0000]\n",
      "        Fact         [0.0861] |     Interesting      [0.0006]\n",
      "         of          [0.6880] |        choice        [0.0002]\n",
      "        the          [0.0280] |          of          [0.0001]\n",
      "        and          [0.1063] |        words         [0.0001]\n",
      "        from         [0.0229] |         ...          [0.0001]\n",
      "       there         [0.0987] |         Are          [0.0000]\n",
      "      familiar       [0.1036] |         you          [0.0000]\n",
      "        ming         [0.9967] |        confir        [0.0000]\n",
      "        the          [0.1262] |         ming         [0.0000]\n",
      "        the          [0.1689] |         that         [0.0017]\n",
      "        and          [0.0951] |     governments      [0.0004]\n",
      "         ed          [0.6149] |         fund         [0.0007]\n",
      "        and          [0.0462] |                      [0.0028]\n",
      "       Terror        [0.0539] |          ⁇           [0.0004]\n",
      "       Terror        [0.1092] |         ter          [0.0000]\n",
      "        tern         [0.1156] |         ror          [0.0022]\n",
      "     terrorism       [0.2509] |         ism          [0.0000]\n",
      "   international     [0.0367] |          ?           [0.0000]\n",
      "       minded        [0.1049] |         Bit          [0.0000]\n",
      "        ways         [0.0513] |          of          [0.0011]\n",
      "     terrorism       [0.2129] |          an          [0.0019]\n",
      "     plausible       [0.0342] |         open         [0.0045]\n",
      "         a           [0.0420] |         door         [0.0002]\n",
      "     terrorism       [0.0289] |          ,           [0.0000]\n"
     ]
    }
   ],
   "source": [
    "decode(lm, base_dataset.table.Tweet.iloc[5], example_id=5, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@POLITICOEurope Interesting choice of words... Are you confirming that governments fund #terrorism? Bit of an open door, but still...'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dataset.table.Tweet.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/51 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (64) to match target batch_size (3648).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a448a7fdc805>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mln\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mppls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2007\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2009\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1834\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m-> 1836\u001b[0;31m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[1;32m   1837\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (64) to match target batch_size (3648)."
     ]
    }
   ],
   "source": [
    "ppls = []\n",
    "for txt, ln in tqdm(train_dl):\n",
    "    txt = txt.to(device)\n",
    "    x = txt[:, :-1]\n",
    "    y = txt[:, 1:]\n",
    "    ln = ln.to(device) - 1\n",
    "    with torch.no_grad():\n",
    "        feats, outs = teacher(x, ln)\n",
    "        loss = F.cross_entropy(outs, y.reshape(-1), ignore_index=0)\n",
    "        ppls.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = txt.to(device)\n",
    "ln = ln.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "feats, outs = teacher(txt, ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.vocab_size = 32001\n",
    "args.hidden_size = 128\n",
    "args.blocks_size = 256\n",
    "args.n_blocks = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.randint(32001, size=(55, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([55, 32])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResConvBlock(nn.Module):\n",
    "        \n",
    "    def __init__(self, input_size, output_size, kernel_size=3, activation=F.gelu):\n",
    "        super(ResConvBlock, self).__init__()\n",
    "        self.perform_residual = (input_size == output_size)\n",
    "        self.activation = activation\n",
    "\n",
    "        self.cnn = nn.Conv1d(in_channels=input_size, out_channels=output_size, kernel_size=kernel_size, padding=(kernel_size // 2))\n",
    "        self.cnn_ln = nn.LayerNorm(output_size)\n",
    "            \n",
    "        self.ff = nn.Linear(in_features=output_size, out_features=output_size)\n",
    "        self.ff_ln = nn.LayerNorm(output_size)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        X is a tensor of shape [TimeSteps x BatchSize x InputSize]\n",
    "        :return Tensor of shape [TimeSteps x BatchSize x OutputSize]\n",
    "        \"\"\"\n",
    "        if self.perform_residual:\n",
    "            residual = x\n",
    "            x = x.permute(1, 2, 0) ## [Time x Batch x Embedding] => [Batch x Embedding x Time]\n",
    "            x = self.activation(self.cnn(x))\n",
    "            x = x.permute(2, 0, 1) ## [Batch x Embedding x Time] => [Time x Batch x Embedding]\n",
    "            x = self.cnn_ln(residual + x)\n",
    "        else:\n",
    "            x = x.permute(1, 2, 0) ## [Time x Batch x Embedding] => [Batch x Embedding x Time]\n",
    "            x = self.activation(self.cnn(x))\n",
    "            x = x.permute(2, 0, 1) ## [Batch x Embedding x Time] => [Time x Batch x Embedding]\n",
    "                \n",
    "        residual = x\n",
    "        x = self.ff(x)\n",
    "        x = self.ff_ln(residual + x)\n",
    "            \n",
    "        return x\n",
    "        \n",
    "class DistillatedLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(DistillatedLanguageModel, self).__init__()\n",
    "        self.embed = nn.Embedding(args.vocab_size, args.hidden_size, padding_idx=0)\n",
    "        \n",
    "        self.entry_block = ResConvBlock(input_size=args.hidden_size, output_size=args.blocks_size)\n",
    "        \n",
    "        self.blocks = nn.ModuleList(modules=[ResConvBlock(input_size=args.blocks_size, output_size=args.blocks_size) for _ in range(args.n_blocks)])\n",
    "        \n",
    "        self.out_project = nn.Linear(in_features=args.blocks_size, out_features=args.hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        \n",
    "        x = self.entry_block(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.out_project(x)\n",
    "        \n",
    "        return F.linear(x, self.embed.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistillatedLanguageModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, lens = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([74, 64])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([74, 64])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_out = model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, teacher_out = teacher(text, lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([74, 64, 32001]), torch.Size([74, 64, 32001]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_out.shape, teacher_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_probs = F.softmax(student_out/temp, -1)\n",
    "teacher_probs = F.softmax(teacher_out/temp, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.3630e-10, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(student_probs, teacher_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.3630e-10, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

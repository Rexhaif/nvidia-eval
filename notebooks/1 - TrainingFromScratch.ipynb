{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils import data as D\n",
    "\n",
    "from radam import RAdam\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from data_utils.preprocess import process_tweet, batch_tokens\n",
    "from data_utils.tokenization import SentencePieceTokenizer\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SentencePieceTokenizer(model_path=\"../models/ama_32k_tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationDataset(D.Dataset):\n",
    "    def __init__(self, text_csv_file: str, text_column: str, tokenizer: SentencePieceTokenizer, preprocess_fn, maxlen=144):\n",
    "        super(DistillationDataset, self).__init__()\n",
    "        self.table: pd.Dataset = pd.read_csv(text_csv_file, memory_map=True)\n",
    "        \n",
    "        self.column = text_column\n",
    "        self.tokenizer = tokenizer\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        self.maxlen = maxlen\n",
    "     \n",
    "    def tokenize(self, tokenizer, text):\n",
    "        \"\"\"\n",
    "        Tokenizes a text using SentencePiece tokenizer\n",
    "        \"\"\"\n",
    "        input_text = self.tokenizer.EncodeAsIds(text, self.preprocess_fn).tokenization\n",
    "    \n",
    "        return input_text\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_collate_fn():\n",
    "        def batch_tokens(token_lists, tensor_type=torch.LongTensor, fill_value=0):\n",
    "            lens = torch.from_numpy(np.array(list(map(len, token_lists)), dtype=np.int64))\n",
    "            x_tensor = fill_value * torch.ones(len(lens), max(lens)).type(tensor_type)\n",
    "            y_tensor = fill_value * torch.ones(len(lens), max(lens)).type(tensor_type)\n",
    "            for i, string in enumerate(token_lists):\n",
    "                _tokenize_str(string[:-1], tensor_type, x_tensor[i])\n",
    "                _tokenize_str(string[1:], tensor_type, y_tensor[i])\n",
    "            return x_tensor, y_tensor\n",
    "\n",
    "        def _tokenize_str(data, tensor_type, char_tensor=None):\n",
    "            \"\"\"\n",
    "            Parses a utf-8 encoded string and assigns to ByteTensor char_tensor.\n",
    "            If no char_tensor is provide one is created.\n",
    "            Typically used internally by `tokenize_str_batch`.\n",
    "            \"\"\"\n",
    "            if char_tensor is None:\n",
    "                if isinstance(data, str):\n",
    "                    # data could either be a string or a list of ids.\n",
    "                    data = data.encode()\n",
    "                char_tensor = tensor_type(len(data))\n",
    "            for i, char in enumerate(data):\n",
    "                char_tensor[i] = char\n",
    "                \n",
    "        return batch_tokens\n",
    "\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.table)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> List[str]:\n",
    "        sample = self.table[self.column].iloc[idx]\n",
    "        sample = self.tokenize(self.tokenizer, sample)\n",
    "        if len(sample) > self.maxlen:\n",
    "            sample = sample[:self.maxlen]\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset = DistillationDataset(\"../data/batch_1.csv\", 'text', tokenizer=tokenizer, preprocess_fn=process_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = D.Subset(base_dataset, np.arange(0, 950000)), \\\n",
    "                     D.Subset(base_dataset, np.arange(950000, 975000)), \\\n",
    "                     D.Subset(base_dataset, np.arange(975000, len(base_dataset) // 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.learning_rate = 0.0001\n",
    "args.vocab_size = 32001\n",
    "args.hidden_size = 64\n",
    "args.blocks_size = 64\n",
    "args.n_blocks = 24\n",
    "args.batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResConvBlock(nn.Module):\n",
    "        \n",
    "    def __init__(self, input_size, output_size, kernel_size=3, activation=F.gelu):\n",
    "        super(ResConvBlock, self).__init__()\n",
    "        self.perform_residual = (input_size == output_size)\n",
    "        self.activation = activation\n",
    "\n",
    "        self.cnn = nn.Conv1d(in_channels=input_size, out_channels=output_size, kernel_size=kernel_size, padding=(kernel_size // 2))\n",
    "        self.cnn_ln = nn.LayerNorm(output_size)\n",
    "            \n",
    "        self.ff = nn.Linear(in_features=output_size, out_features=output_size)\n",
    "        self.ff_ln = nn.LayerNorm(output_size)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        X is a tensor of shape [TimeSteps x BatchSize x InputSize]\n",
    "        :return Tensor of shape [TimeSteps x BatchSize x OutputSize]\n",
    "        \"\"\"\n",
    "        if self.perform_residual:\n",
    "            residual = x\n",
    "            x = x.permute(1, 2, 0) ## [Time x Batch x Embedding] => [Batch x Embedding x Time]\n",
    "            x = self.activation(self.cnn(x))\n",
    "            x = x.permute(2, 0, 1) ## [Batch x Embedding x Time] => [Time x Batch x Embedding]\n",
    "            x = self.cnn_ln(residual + x)\n",
    "        else:\n",
    "            x = x.permute(1, 2, 0) ## [Time x Batch x Embedding] => [Batch x Embedding x Time]\n",
    "            x = self.activation(self.cnn(x))\n",
    "            x = x.permute(2, 0, 1) ## [Batch x Embedding x Time] => [Time x Batch x Embedding]\n",
    "                \n",
    "        residual = x\n",
    "        x = self.ff(x)\n",
    "        x = self.ff_ln(residual + x)\n",
    "            \n",
    "        return x\n",
    "        \n",
    "class ConvLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(ConvLanguageModel, self).__init__()\n",
    "        self.embed = nn.Embedding(args.vocab_size, args.hidden_size, padding_idx=0)\n",
    "        \n",
    "        self.entry_block = ResConvBlock(input_size=args.hidden_size, output_size=args.blocks_size)\n",
    "        \n",
    "        self.blocks = nn.ModuleList(modules=[ResConvBlock(input_size=args.blocks_size, output_size=args.blocks_size) for _ in range(args.n_blocks)])\n",
    "        \n",
    "        self.out_project = nn.Linear(in_features=args.blocks_size, out_features=args.hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        \n",
    "        x = self.entry_block(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.out_project(x)\n",
    "        \n",
    "        return F.linear(x, self.embed.weight)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.embed(x)\n",
    "        \n",
    "        x = self.entry_block(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        x = self.out_project(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModellingModel(pl.LightningModule):\n",
    "    def __init__(self, args, train_dataset, valid_dataset, test_dataset):\n",
    "        super(LanguageModellingModel, self).__init__()\n",
    "        self.args = args\n",
    "        self.train_ds = train_dataset\n",
    "        self.valid_ds = valid_dataset\n",
    "        self.test_ds  = test_dataset\n",
    "        \n",
    "        self.model = ConvLanguageModel(self.args)\n",
    "        self.loss = nn.CrossEntropyLoss(ignore_index=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        lr = self.args.learning_rate\n",
    "        opt = RAdam(self.model.parameters(), lr=lr)\n",
    "        return opt\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Lightning calls this inside the training loop\n",
    "        :param batch:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # forward pass\n",
    "        x_in, x_out = batch\n",
    "        \n",
    "        x_out = x_out.view(-1)\n",
    "\n",
    "        x_hat = self.forward(x_in)\n",
    "\n",
    "        # calculate loss\n",
    "        loss_val = self.loss(x_hat.view(-1, self.args.vocab_size), x_out)\n",
    "        \n",
    "        ppl_val = torch.exp(loss_val)\n",
    "\n",
    "        tqdm_dict = {'train_loss': loss_val, 'train_ppl': ppl_val}\n",
    "        output = {\n",
    "            'loss': loss_val,\n",
    "            'progress_bar': tqdm_dict,\n",
    "            'log': tqdm_dict\n",
    "        }\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Lightning calls this inside the validation loop\n",
    "        :param batch:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # forward pass\n",
    "        x_in, x_out = batch\n",
    "        \n",
    "        x_out = x_out.view(-1)\n",
    "        \n",
    "        #with torch.no_grad():\n",
    "        x_hat = self.forward(x_in)\n",
    "            # calculate loss\n",
    "        loss_val = self.loss(x_hat.view(-1, self.args.vocab_size), x_out)\n",
    "        ppl_val = torch.exp(loss_val)\n",
    "\n",
    "        output = {'val_loss': loss_val, 'val_ppl': ppl_val}\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def validation_end(self, outputs):\n",
    "        \"\"\"\n",
    "        Called at the end of validation to aggregate outputs\n",
    "        :param outputs: list of individual outputs of each validation step\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        val_losses = []\n",
    "        val_pplxs = []\n",
    "        for output in outputs:\n",
    "            val_loss = output['val_loss'].item()\n",
    "            val_ppl = output['val_ppl'].item()\n",
    "            \n",
    "            val_losses.append(val_loss)\n",
    "            val_pplxs.append(val_ppl)\n",
    "        \n",
    "        mean_loss = np.mean(val_losses)\n",
    "        std_loss = np.std(val_losses)\n",
    "        \n",
    "        mean_ppl = np.mean(val_pplxs)\n",
    "        std_ppl = np.std(val_pplxs)\n",
    "\n",
    "        tqdm_dict = {'val_loss': mean_loss, 'val_ppl': mean_ppl, 'val_loss_std': std_loss, 'val_ppl_std': std_ppl}\n",
    "        result = {'progress_bar': tqdm_dict, 'log': tqdm_dict, 'val_loss': mean_loss, 'val_ppl': mean_ppl}\n",
    "        return result\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        #print('training data loader called')\n",
    "        return D.DataLoader(self.train_ds, batch_size=args.batch_size, collate_fn=DistillationDataset.get_collate_fn())\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        #print('val data loader called')\n",
    "        return D.DataLoader(self.valid_ds, batch_size=args.batch_size, collate_fn=DistillationDataset.get_collate_fn())\n",
    "\n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        #print('test data loader called')\n",
    "        return D.DataLoader(self.test_ds, batch_size=args.batch_size, collate_fn=DistillationDataset.get_collate_fn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_module = LanguageModellingModel(args, train, valid, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "es = pl.callbacks.EarlyStopping(monitor='val_ppl', min_delta=0.001, patience=20, mode='min')\n",
    "ms = pl.callbacks.ModelCheckpoint(f\"./models/{dt.now().date}\", monitor='val_ppl', save_best_only=True, save_weights_only=True, mode='min', prefix='cnn_6blocks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu available: True, used: True\n",
      "VISIBLE GPUS: 0\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(min_nb_epochs=5, max_nb_epochs=1000, checkpoint_callback=ms, early_stop_callback=es, gpus=[0], log_gpu_memory='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30470 [00:00<19:54, 25.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Name               Type Params\n",
      "0                       model  ConvLanguageModel    2 M\n",
      "1                 model.embed          Embedding    2 M\n",
      "2           model.entry_block       ResConvBlock   16 K\n",
      "3       model.entry_block.cnn             Conv1d   12 K\n",
      "4    model.entry_block.cnn_ln          LayerNorm  128  \n",
      "..                        ...                ...    ...\n",
      "125    model.blocks.23.cnn_ln          LayerNorm  128  \n",
      "126        model.blocks.23.ff             Linear    4 K\n",
      "127     model.blocks.23.ff_ln          LayerNorm  128  \n",
      "128         model.out_project             Linear    4 K\n",
      "129                      loss   CrossEntropyLoss    0  \n",
      "\n",
      "[130 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 27927/30470 [42:54<03:44, 11.31it/s, batch_nb=27926, epoch=3, gpu=0, loss=2.304, train_loss=3.19, train_ppl=24.4, v_nb=7, val_loss=2.17, val_loss_std=0.668, val_ppl=10.9, val_ppl_std=7.69] "
     ]
    }
   ],
   "source": [
    "trainer.fit(lm_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
